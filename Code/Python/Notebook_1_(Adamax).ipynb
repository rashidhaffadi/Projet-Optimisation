{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o6tX6HM8uC3F"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lnPaxtjluC4W"
   },
   "source": [
    "## Multivariate case\n",
    "\n",
    "We are trying to minimise $\\sum \\xi_i^2$. This time $ f = Xw$ where $w$ is Dx1 and $X$ is NxD.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L} & = \\frac{1}{N} (y-Xw)^T(y-Xw) \\\\\n",
    "\\frac{\\delta\\mathcal{L}}{\\delta w} & = -\\frac{1}{N} 2\\left(\\frac{\\delta f(X,w)}{\\delta w}\\right)^T(y-Xw) \\\\ \n",
    "& = -\\frac{2}{N} \\left(\\frac{\\delta f(X,w)}{\\delta w}\\right)^T\\xi\n",
    "\\end{align}\n",
    "where $\\xi_i$ is the error term $y_i-f(X,w)$ and \n",
    "$$\n",
    "\\frac{\\delta f(X,w)}{\\delta w} = X\n",
    "$$\n",
    "\n",
    "Finally the weights can be updated as $w_{new} = w_{current} - \\gamma \\frac{\\delta\\mathcal{L}}{\\delta w}$ where $\\gamma$ is a learning rate between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L1LyM0TUuC4Y"
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "D = 5\n",
    "X = 5*np.random.randn(N,D)\n",
    "w_true = np.random.randn(D,1)\n",
    "y = X.dot(w_true)\n",
    "y_obs = y + np.random.randn(N,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "wl6w5KwouC5q",
    "outputId": "eaa0e893-9867-48f0-fe1b-220a9df58523"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06470447],\n",
       "       [ 1.4400372 ],\n",
       "       [-1.15309585],\n",
       "       [ 0.63568265],\n",
       "       [ 0.76527259]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X,y)\n",
    "model.coef_.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GgJa5CAMuC59"
   },
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dL_dw(X,e,w):\n",
    "    return -2*e*X.T/len(X)\n",
    "\n",
    "def u_choice(bet, un, sgC):\n",
    "  u_norm = max(bet**2*un.T.dot(un) , sgC.T.dot(sgC))\n",
    "  if u_norm ==  bet**2*un.T.dot(un) :\n",
    "    return bet*un\n",
    "  elif u_norm ==  abs(sgC.T.dot(sgC)) :\n",
    "    return abs(sgC)\n",
    "  else :\n",
    "    return \"Erreur in selecting u\"\n",
    "\n",
    "def loss_function(e):\n",
    "    L = e.T.dot(e)/N\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2oeytvXXxRr4"
   },
   "outputs": [],
   "source": [
    "\n",
    "def adamax(X,y_obs,alpha=0.5, beta1 = 0.9, beta2=0.999,batch_size = 25, epoch = 10):\n",
    "    #Initialisation des param√©tres \n",
    "    N = len(X)\n",
    "    D = len(X[0])\n",
    "    # les conditions initiales\n",
    "    w = np.random.randn(D,1)\n",
    "    params = []\n",
    "    loss = np.zeros((N,1))\n",
    "    Loss = np.zeros((N,1))\n",
    "    m = np.zeros((D,1))\n",
    "    u = np.zeros((D,1))\n",
    "    betai = 1\n",
    "    for b in range(epoch):\n",
    "        for i in range(batch_size):\n",
    "            params.append(w)\n",
    "            idx = np.random.choice(N,batch_size,replace=False)\n",
    "            e = y_obs[idx] - X[idx].dot(w)\n",
    "            #just for testing\n",
    "            ##########################\n",
    "            e_global = y_obs - X.dot(w)\n",
    "            L = loss_function(e_global)\n",
    "            ##########################\n",
    "            sg = dL_dw(X[idx],e,w)\n",
    "            m = beta1*m + (1-beta1)*sg\n",
    "            u = u_choice(beta2,u,sg)\n",
    "            betai = betai*beta1\n",
    "            mHat = (m/(1-betai))\n",
    "            w = w - alpha*mHat/u\n",
    "            #calculer erreur (e) avec les nouvelles configs\n",
    "            e = y_obs[idx] - X[idx].dot(w)\n",
    "            loss[i] = e\n",
    "            Loss[i] = L\n",
    "    print(\"epoch\",b, \": Loss = \", L, sep=\" \")\n",
    "        \n",
    "    return params,loss,Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (25,1) (5,25) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-0a1c9b8a1d65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLoss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_obs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#Test avec X et y_obs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-90cb23509239>\u001b[0m in \u001b[0;36madamax\u001b[1;34m(X, y_obs, alpha, beta1, beta2, batch_size, epoch)\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me_global\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;31m##########################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdL_dw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mm\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mu_choice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-8fb7bfd935e7>\u001b[0m in \u001b[0;36mdL_dw\u001b[1;34m(X, e, w)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdL_dw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mu_choice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[0mu_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbet\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mun\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0msgC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msgC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (25,1) (5,25) "
     ]
    }
   ],
   "source": [
    "params, loss, Loss = adamax(X,y_obs)#Test avec X et y_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "z9rjeQVNuC6E",
    "outputId": "2a07fdea-48ee-4173-b30d-29905bb555b2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-e9d224f1f5ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLoss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(loss)\n",
    "plt.plot(Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.299356]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.0273531])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(params[-1])\n",
    "Loss[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.299356  , 0.23879586]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare parameters side by side\n",
    "np.hstack([params[-1],model.coef_.T])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Notebook 1 (Adamax).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
